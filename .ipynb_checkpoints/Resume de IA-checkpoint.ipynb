{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7a4dd0ae760aa13c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T13:15:55.945388Z",
     "start_time": "2025-04-07T13:15:55.924256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"78e9a445-6e97-4a8a-8f95-03c430c9882b\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Cuaderno Extendido: Teoría y Ejemplos en IA Sub-Simbólica\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este cuaderno reúne de manera detallada los fundamentos teóricos y prácticos de varios temas clave en IA sub-simbólica. Se abordan:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Distribuciones Gaussianas (unidimensional y multivariante)\\n\",\n",
    "    \"- Reconocimiento de formas y extracción de características\\n\",\n",
    "    \"- Estimación no paramétrica de densidades: método de Ventana de Parzen y k-Nearest Neighbors (k-NN)\\n\",\n",
    "    \"- Estimación paramétrica de densidades (máxima verosimilitud)\\n\",\n",
    "    \"- Uso de redes neuronales para estimar densidades (Parzen Neural Networks – PNN)\\n\",\n",
    "    \"- Fundamentos de las redes neuronales artificiales (ANN)\\n\",\n",
    "    \"- Clasificadores bayesianos y toma de decisiones\\n\",\n",
    "    \"\\n\",\n",
    "    \"Cada sección contiene una explicación teórica completa y ejemplos de código con su respectiva justificación, para que puedas comprender en profundidad cada tema.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"cf6d113c-5b32-4e0f-b0fe-0b6d142b25b5\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Distribuciones Gaussianas y sus Propiedades\\n\",\n",
    "    \"\\n\",\n",
    "    \"Las distribuciones gaussianas (o normales) son esenciales en estadística y en el procesamiento de señales. \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Distribución Normal Unidimensional:\\n\",\n",
    "    \"\\n\",\n",
    "    \"La función de densidad de probabilidad (pdf) de una variable aleatoria \\\\(X\\\\) con distribución normal es:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"p(x) = \\\\frac{1}{\\\\sigma \\\\sqrt{2\\\\pi}} \\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"donde \\\\(\\\\mu\\\\) es la media y \\\\(\\\\sigma^2\\\\) es la varianza.\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Distribución Normal Multivariante:\\n\",\n",
    "    \"\\n\",\n",
    "    \"Para un vector aleatorio \\\\(x \\\\in \\\\mathbb{R}^d\\\\), la pdf es:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"p(x) = \\\\frac{1}{\\\\sqrt{(2\\\\pi)^d |\\\\Sigma|}} \\\\exp\\\\left(-\\\\frac{1}{2}(x-\\\\mu)^T\\\\Sigma^{-1}(x-\\\\mu)\\\\right)\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"donde \\\\(\\\\mu\\\\) es el vector de medias y \\\\(\\\\Sigma\\\\) la matriz de covarianza. Los contornos de nivel (donde \\\\(p(x)\\\\) es constante) son elipses (o hiperelipsoides) que se alinean con los autovectores de \\\\(\\\\Sigma\\\\).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"Conocer las propiedades de la distribución normal es fundamental para modelar la incertidumbre en los datos, estimar parámetros en modelos paramétricos y construir clasificadores basados en la distancia (como los basados en la distancia de Mahalanobis).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"04c95d2b-1e32-42e2-93a0-0f64b6d846aa\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Ejemplo 1: Visualización de una distribución normal unidimensional\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Parámetros de la distribución normal\\n\",\n",
    "    \"mu = 0\\n\",\n",
    "    \"sigma = 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generamos un rango de valores para x\\n\",\n",
    "    \"x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\\n\",\n",
    "    \"pdf = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-((x-mu)**2)/(2*sigma**2))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8, 4))\\n\",\n",
    "    \"plt.plot(x, pdf, label='N(0,1)')\\n\",\n",
    "    \"plt.title('Distribución Normal Unidimensional')\\n\",\n",
    "    \"plt.xlabel('x')\\n\",\n",
    "    \"plt.ylabel('Densidad')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Este ejemplo muestra cómo se comporta una normal estándar, lo cual es la base para muchos modelos.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e2d3c453-f640-4f27-85ef-35c1deceda45\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Ejemplo 2: Contornos de una Distribución Gaussiana Multivariante (2D)\\n\",\n",
    "    \"\\n\",\n",
    "    \"En 2D, los contornos de la densidad son elipses. Estos contornos están determinados por la matriz de covarianza y reflejan la correlación entre las variables. En el siguiente ejemplo se plotean los contornos de una distribución 2D usando una elipse que se calcula a partir de los autovalores y autovectores de \\\\(\\\\Sigma\\\\).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"8f742733-1d38-4bd0-a5d6-90fa43e78033\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from matplotlib.patches import Ellipse\\n\",\n",
    "    \"\\n\",\n",
    "    \"def plot_gaussian_contours(mu, Sigma, ax, n_std=2.0, facecolor='none', **kwargs):\\n\",\n",
    "    \"    # Calcula autovalores y autovectores\\n\",\n",
    "    \"    vals, vecs = np.linalg.eigh(Sigma)\\n\",\n",
    "    \"    order = vals.argsort()[::-1]\\n\",\n",
    "    \"    vals, vecs = vals[order], vecs[:, order]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ángulo de rotación\\n\",\n",
    "    \"    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ancho y alto de la elipse\\n\",\n",
    "    \"    width, height = 2 * n_std * np.sqrt(vals)\\n\",\n",
    "    \"    ellip = Ellipse(xy=mu, width=width, height=height, angle=theta, facecolor=facecolor, **kwargs)\\n\",\n",
    "    \"    ax.add_patch(ellip)\\n\",\n",
    "    \"    return ellip\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Parámetros para la gaussiana 2D\\n\",\n",
    "    \"mu2d = np.array([1, 2])\\n\",\n",
    "    \"Sigma2d = np.array([[3, 1], [1, 2]])\\n\",\n",
    "    \"\\n\",\n",
    "    \"x, y = np.mgrid[-2:6:.01, -1:7:.01]\\n\",\n",
    "    \"pos = np.dstack((x, y))\\n\",\n",
    "    \"\\n\",\n",
    "    \"def multivariate_gaussian(pos, mu, Sigma):\\n\",\n",
    "    \"    n = mu.shape[0]\\n\",\n",
    "    \"    Sigma_det = np.linalg.det(Sigma)\\n\",\n",
    "    \"    Sigma_inv = np.linalg.inv(Sigma)\\n\",\n",
    "    \"    N = np.sqrt((2*np.pi)**n * Sigma_det)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\\n\",\n",
    "    \"    return np.exp(-fac/2) / N\\n\",\n",
    "    \"\\n\",\n",
    "    \"Z = multivariate_gaussian(pos, mu2d, Sigma2d)\\n\",\n",
    "    \"\\n\",\n",
    "    \"fig, ax = plt.subplots(figsize=(8, 6))\\n\",\n",
    "    \"cf = ax.contourf(x, y, Z, cmap='viridis')\\n\",\n",
    "    \"plt.colorbar(cf)\\n\",\n",
    "    \"plot_gaussian_contours(mu2d, Sigma2d, ax, n_std=2, edgecolor='red')\\n\",\n",
    "    \"ax.set_title('Contornos de una Distribución Gaussiana 2D')\\n\",\n",
    "    \"ax.set_xlabel('x')\\n\",\n",
    "    \"ax.set_ylabel('y')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Este código muestra visualmente cómo la matriz de covarianza afecta la forma de la distribución.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"3164cfed-ff85-42dc-9691-2f485d46f341\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Reconocimiento de Formas y Extracción de Características\\n\",\n",
    "    \"\\n\",\n",
    "    \"En sistemas de IA, el reconocimiento de patrones implica convertir señales del mundo real en vectores de características que luego se usan para clasificar o identificar objetos. \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"El proceso general incluye:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Percepción:** Se capta el estímulo (por ejemplo, una imagen o un sonido) y se digitaliza.\\n\",\n",
    "    \"2. **Extracción de Características:** Se procesan los datos para extraer información relevante (por ejemplo, contornos, píxeles negros, medidas geométricas, etc.).\\n\",\n",
    "    \"3. **Clasificación:** Se utiliza un clasificador (por ejemplo, basado en redes neuronales o métodos estadísticos) para asignar la entrada a una clase.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"La calidad de las características extraídas es crucial; características bien elegidas permiten reducir la dimensionalidad del problema y mejorar la precisión del sistema de reconocimiento.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"A continuación se simula la extracción de una característica sencilla (por ejemplo, la intensidad media) a partir de datos sintéticos para dos clases.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"1f8f33d8-8233-44d9-a4af-0035e8bcae53\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Ejemplo: Extracción de una característica y visualización de un histograma\\n\",\n",
    "    \"np.random.seed(0)\\n\",\n",
    "    \"n_samples = 100\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Simulamos dos clases con diferentes medias\\n\",\n",
    "    \"features_class1 = np.random.normal(loc=3, scale=0.5, size=(n_samples, 1))\\n\",\n",
    "    \"features_class2 = np.random.normal(loc=7, scale=0.5, size=(n_samples, 1))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Etiquetas\\n\",\n",
    "    \"labels_class1 = np.zeros((n_samples,))\\n\",\n",
    "    \"labels_class2 = np.ones((n_samples,))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Unimos los datos\\n\",\n",
    "    \"X_features = np.vstack((features_class1, features_class2))\\n\",\n",
    "    \"y_labels = np.hstack((labels_class1, labels_class2))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8,4))\\n\",\n",
    "    \"plt.hist(features_class1, bins=20, alpha=0.7, label='Clase 1')\\n\",\n",
    "    \"plt.hist(features_class2, bins=20, alpha=0.7, label='Clase 2')\\n\",\n",
    "    \"plt.title('Histograma de la característica extraída')\\n\",\n",
    "    \"plt.xlabel('Valor de la característica')\\n\",\n",
    "    \"plt.ylabel('Frecuencia')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: Este ejemplo ilustra cómo, aun una característica simple, puede separar dos clases.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e9ab5c3e-26a8-4413-b73d-d7afc4e04a38\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Estimación No Paramétrica de Densidades: Ventana de Parzen\\n\",\n",
    "    \"\\n\",\n",
    "    \"La estimación de densidad no paramétrica permite calcular \\\\(p(x)\\\\) directamente a partir de los datos sin asumir una forma fija para la distribución. \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"El método de la Ventana de Parzen consiste en definir un kernel (por ejemplo, un indicador de un intervalo o un kernel gaussiano) y estimar:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"p(x_0) \\\\approx \\\\frac{k}{n\\\\, V},\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"donde \\\\(k\\\\) es el número de muestras que caen en la ventana centrada en \\\\(x_0\\\\), \\\\(n\\\\) es el número total de muestras, y \\\\(V\\\\) es el volumen (en 1D, el ancho) de la ventana.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este método es muy flexible, ya que no impone una forma predefinida a la pdf, pero su desempeño depende fuertemente de la elección del ancho \\\\(h\\\\) del kernel.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se implementa una función en 1D que calcula la densidad usando un kernel de ventana rectangular.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"3cbdebf5-3235-45ff-a78a-7e18b9fbb1ac\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def parzen_window_estimator(x0, data, h):\\n\",\n",
    "    \"    # Ventana rectangular de ancho h (volumen en 1D es h)\\n\",\n",
    "    \"    n = len(data)\\n\",\n",
    "    \"    kernel_values = np.where(np.abs(data - x0) <= h/2, 1, 0)\\n\",\n",
    "    \"    return np.sum(kernel_values) / (n * h)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generamos datos sintéticos: mezcla de dos distribuciones\\n\",\n",
    "    \"data_parzen = np.concatenate((np.random.normal(2, 0.3, 500), np.random.normal(5, 0.5, 500)))\\n\",\n",
    "    \"x_vals = np.linspace(0, 7, 200)\\n\",\n",
    "    \"density_est_parzen = [parzen_window_estimator(x, data_parzen, h=0.5) for x in x_vals]\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8,4))\\n\",\n",
    "    \"plt.plot(x_vals, density_est_parzen, label='Estimación Parzen')\\n\",\n",
    "    \"plt.title('Estimación de densidad usando Ventana de Parzen')\\n\",\n",
    "    \"plt.xlabel('x')\\n\",\n",
    "    \"plt.ylabel('Densidad estimada')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: La elección de h afecta la suavidad de la estimación. Si h es muy grande se promedia en exceso, y si es muy pequeño la estimación es ruidosa.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f88bb3e8-45d9-4d66-bb8d-d3f93bb2c0a1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Estimación No Paramétrica: k-Nearest Neighbors (k-NN)\\n\",\n",
    "    \"\\n\",\n",
    "    \"El método k-NN para estimación de densidad fija un número de vecinos \\\\(k\\\\) y determina el volumen \\\\(V\\\\) mínimo que contiene esos vecinos. La densidad se estima como:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"p(x_0) \\\\approx \\\\frac{k}{n\\\\, V},\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se debe elegir \\\\(k\\\\) de modo que \\\\(k \\\\to \\\\infty\\\\) y \\\\(k/n \\\\to 0\\\\) cuando \\\\(n \\\\to \\\\infty\\\\), lo que garantiza la consistencia del estimador.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"La ventaja del k-NN es que adapta el volumen de estimación a la densidad local de los datos. En regiones de alta densidad, el volumen será pequeño, lo que mejora la precisión de la estimación.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se implementa una función para estimar la densidad en 1D usando el método k-NN.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"7a82e3a9-2a55-4c9a-9a0a-4b63bdc5b67d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def knn_density_estimator(x0, data, k):\\n\",\n",
    "    \"    # Calcula la distancia absoluta a todos los puntos\\n\",\n",
    "    \"    distances = np.abs(data - x0)\\n\",\n",
    "    \"    sorted_distances = np.sort(distances)\\n\",\n",
    "    \"    # En 1D, el 'volumen' es 2 * la k-ésima distancia\\n\",\n",
    "    \"    radius = sorted_distances[k-1]\\n\",\n",
    "    \"    V = 2 * radius\\n\",\n",
    "    \"    return k / (len(data) * V)\\n\",\n",
    "    \"\\n\",\n",
    "    \"k_val = int(np.sqrt(len(data_parzen)))\\n\",\n",
    "    \"density_est_knn = [knn_density_estimator(x, data_parzen, k_val) for x in x_vals]\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8,4))\\n\",\n",
    "    \"plt.plot(x_vals, density_est_knn, label=f'k-NN (k={k_val})', color='orange')\\n\",\n",
    "    \"plt.title('Estimación de densidad usando k-Nearest Neighbors')\\n\",\n",
    "    \"plt.xlabel('x')\\n\",\n",
    "    \"plt.ylabel('Densidad estimada')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: k-NN adapta la ventana al nivel local de densidad, mejorando la estimación en regiones de alta variabilidad.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"d329f3b2-75a7-4a72-8884-fb8b27597270\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Estimación Paramétrica de Densidades\\n\",\n",
    "    \"\\n\",\n",
    "    \"En el enfoque paramétrico se asume que la forma de la pdf es conocida (por ejemplo, gaussiana) y se estima sus parámetros (media, varianza, etc.) mediante el método de máxima verosimilitud (ML).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"Para una distribución normal unidimensional, la función de verosimilitud dada \\\\(n\\\\) muestras \\\\(y_1,\\\\ldots,y_n\\\\) es:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"L(\\\\mu,\\\\sigma^2) = \\\\prod_{k=1}^{n} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{(y_k-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"Tomando el logaritmo y derivando se obtiene que el estimador de ML para la media es la media muestral y para la varianza es la varianza muestral.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"Estos estimadores son fundamentales en modelos paramétricos, ya que permiten modelar la pdf con una forma cerrada y obtener soluciones óptimas cuando la suposición es correcta.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se generan datos sintéticos de una normal y se calculan la media y varianza muestrales.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"4dce4d16-f858-431a-8a9f-76329f2d49e0\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Generamos datos sintéticos de una normal\\n\",\n",
    "    \"data_gauss = np.random.normal(loc=5, scale=2, size=1000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Estimación de parámetros (MLE)\\n\",\n",
    "    \"mu_mle = np.mean(data_gauss)\\n\",\n",
    "    \"sigma2_mle = np.mean((data_gauss - mu_mle)**2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Media estimada (MLE): {mu_mle:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Varianza estimada (MLE): {sigma2_mle:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: La media y la varianza muestral son los estimadores de ML para una distribución normal.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c75d31d0-7d36-4b07-a646-9667b9c20cb1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Redes Neuronales para Estimación de Densidades (Parzen Neural Networks – PNN)\\n\",\n",
    "    \"\\n\",\n",
    "    \"La idea es entrenar un perceptrón multicapa (MLP) para aproximar la función de densidad \\\\(p(x)\\\\). Para ello, se generan objetivos (targets) mediante el método de Parzen, y luego se entrena el MLP para que aprenda la relación \\\\(x \\\\to p(x)\\\\).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Se generan datos \\\\(x_i\\\\) de la distribución desconocida.\\n\",\n",
    "    \"2. Para cada \\\\(x_i\\\\), se estima un valor objetivo \\\\(y_i\\\\) usando la Ventana de Parzen.\\n\",\n",
    "    \"3. Se entrena un MLP con pares \\\\((x_i, y_i)\\\\). La salida del MLP será la densidad estimada \\\\(\\\\hat{p}(x)\\\\).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este enfoque permite obtener un modelo compacto de la densidad que generaliza a nuevos datos y reduce la necesidad de almacenar toda la muestra, a diferencia de los métodos puramente basados en memoria (Parzen o k-NN). \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se genera una muestra a partir de una distribución exponencial y se entrena un MLP para aproximar la densidad.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"eb1ac80b-1b46-4a2f-99a8-48d53ab359d9\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow.keras.models import Sequential\\n\",\n",
    "    \"from tensorflow.keras.layers import Dense\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generamos datos de una distribución exponencial\\n\",\n",
    "    \"n_samples = 1000\\n\",\n",
    "    \"data_exp = np.random.exponential(scale=1.0, size=n_samples)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Usamos la función parzen_window_estimator para generar targets\\n\",\n",
    "    \"h_value = 0.2\\n\",\n",
    "    \"X_train = data_exp.reshape(-1, 1)\\n\",\n",
    "    \"y_train = np.array([parzen_window_estimator(x, data_exp, h_value) for x in data_exp])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Definimos un MLP sencillo\\n\",\n",
    "    \"model_pnn = Sequential([\\n\",\n",
    "    \"    Dense(16, activation='relu', input_shape=(1,)),\\n\",\n",
    "    \"    Dense(16, activation='relu'),\\n\",\n",
    "    \"    Dense(1, activation='relu')  # Forzamos salida no negativa\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"model_pnn.compile(optimizer='adam', loss='mse')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Entrenamos el MLP para aproximar la densidad\\n\",\n",
    "    \"history_pnn = model_pnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluamos la densidad estimada en un rango\\n\",\n",
    "    \"x_test = np.linspace(0, np.max(data_exp)*1.1, 200).reshape(-1, 1)\\n\",\n",
    "    \"y_pred = model_pnn.predict(x_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8,4))\\n\",\n",
    "    \"plt.plot(x_test, y_pred, label='Estimación MLP (PNN)')\\n\",\n",
    "    \"parzen_vals = [parzen_window_estimator(x, data_exp, h_value) for x in x_test.flatten()]\\n\",\n",
    "    \"plt.plot(x_test, parzen_vals, '--', label='Parzen Window')\\n\",\n",
    "    \"plt.title('Comparación: PNN vs Ventana de Parzen')\\n\",\n",
    "    \"plt.xlabel('x')\\n\",\n",
    "    \"plt.ylabel('Densidad estimada')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: El MLP aprende una función continua que aproxima la pdf, lo que es ventajoso en cuanto a generalización.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6a57c80a-ec5f-45f6-a817-fdfd680b7a07\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Fundamentos de las Redes Neuronales Artificiales (ANN)\\n\",\n",
    "    \"\\n\",\n",
    "    \"Las redes neuronales son máquinas de procesamiento inspiradas en el cerebro. Sus elementos fundamentales son:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Arquitectura:** Organizada en capas (entrada, ocultas, salida). Cada conexión tiene un peso.\\n\",\n",
    "    \"- **Dinámica:** La señal se propaga a través de la red; cada neurona aplica una función de activación (por ejemplo, sigmoide, ReLU).\\n\",\n",
    "    \"- **Aprendizaje:** Ajuste de los pesos mediante algoritmos como la retropropagación (backpropagation), minimizando una función de error.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"Comprender la teoría detrás de las ANN es crucial para diseñar y entrenar modelos que puedan aproximar funciones complejas, realizar clasificación o regresión, y extraer características de los datos.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"A continuación se entrena un MLP sencillo en el conjunto de datos Iris para clasificar flores.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"62ed5d68-9d20-40a9-8a75-2590f50d4764\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from sklearn.datasets import load_iris\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from tensorflow.keras.utils import to_categorical\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Cargamos el conjunto de datos Iris\\n\",\n",
    "    \"iris = load_iris()\\n\",\n",
    "    \"X_iris = iris.data\\n\",\n",
    "    \"y_iris = iris.target\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Escalamos las características\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"X_scaled = scaler.fit_transform(X_iris)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convertir etiquetas a one-hot\\n\",\n",
    "    \"y_onehot = to_categorical(y_iris)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Dividimos en entrenamiento y prueba\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Definimos un MLP sencillo\\n\",\n",
    "    \"model_iris = Sequential([\\n\",\n",
    "    \"    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\\n\",\n",
    "    \"    Dense(16, activation='relu'),\\n\",\n",
    "    \"    Dense(3, activation='softmax')\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"model_iris.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"history_iris = model_iris.fit(X_train, y_train, epochs=100, batch_size=16, verbose=0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"loss, acc = model_iris.evaluate(X_test, y_test, verbose=0)\\n\",\n",
    "    \"print(f\\\"Precisión en Iris: {acc*100:.2f}%\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: Este ejemplo muestra cómo aplicar el algoritmo de retropropagación para entrenar un MLP en un problema de clasificación real.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"eafcb8ed-6679-430f-bd55-4a18a07a60b8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Clasificadores Bayesianos y Toma de Decisiones\\n\",\n",
    "    \"\\n\",\n",
    "    \"La teoría bayesiana para la clasificación se basa en el Teorema de Bayes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"P(\\\\omega_i|x) = \\\\frac{p(x|\\\\omega_i) P(\\\\omega_i)}{p(x)}\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"La regla de decisión de Bayes asigna un patrón \\\\(x\\\\) a la clase \\\\(\\\\omega_i\\\\) que maximice \\\\(P(\\\\omega_i|x)\\\\).\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Teoría\\n\",\n",
    "    \"\\n\",\n",
    "    \"Si se asume que \\\\(p(x|\\\\omega_i)\\\\) es una pdf gaussiana, se pueden derivar funciones discriminantes. Por ejemplo, tomando el logaritmo:\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\\\[\\n\",\n",
    "    \"g_i(x) = \\\\log p(x|\\\\omega_i) + \\\\log P(\\\\omega_i)\\\\n\",\n",
    "    \"\\\\]\\n\",\n",
    "    \"\\n\",\n",
    "    \"Con pdfs gaussianas, se puede llegar a discriminantes lineales (si se asume covarianza compartida) o cuadráticos.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Importancia\\n\",\n",
    "    \"\\n\",\n",
    "    \"La clasificación bayesiana es óptima en el sentido de minimizar la probabilidad de error. Conocer esta teoría permite diseñar sistemas de clasificación que aprovechan la información probabilística y las estimaciones de densidad.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Ejemplo\\n\",\n",
    "    \"\\n\",\n",
    "    \"Se genera un conjunto de datos 2D para dos clases y se calcula la frontera de decisión usando discriminantes basados en distribuciones gaussianas con covarianza compartida.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"a2949f26-c206-4a53-9d1a-7fdc78b9ea72\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"from matplotlib.colors import ListedColormap\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generamos datos sintéticos para dos clases (2D)\\n\",\n",
    "    \"np.random.seed(1)\\n\",\n",
    "    \"n_points = 200\\n\",\n",
    "    \"mean1 = [2, 2]\\n\",\n",
    "    \"cov1 = [[1, 0.5], [0.5, 1]]\\n\",\n",
    "    \"mean2 = [5, 5]\\n\",\n",
    "    \"cov2 = [[1, -0.3], [-0.3, 1]]\\n\",\n",
    "    \"\\n\",\n",
    "    \"X1 = np.random.multivariate_normal(mean1, cov1, n_points)\\n\",\n",
    "    \"X2 = np.random.multivariate_normal(mean2, cov2, n_points)\\n\",\n",
    "    \"X_bayes = np.vstack((X1, X2))\\n\",\n",
    "    \"y_bayes = np.hstack((np.zeros(n_points), np.ones(n_points)))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Estimación de parámetros para cada clase\\n\",\n",
    "    \"mu1_est = np.mean(X1, axis=0)\\n\",\n",
    "    \"mu2_est = np.mean(X2, axis=0)\\n\",\n",
    "    \"Sigma1_est = np.cov(X1, rowvar=False)\\n\",\n",
    "    \"Sigma2_est = np.cov(X2, rowvar=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Asumimos que ambas clases tienen la misma covarianza (promedio)\\n\",\n",
    "    \"Sigma_shared = (Sigma1_est + Sigma2_est) / 2\\n\",\n",
    "    \"Sigma_inv = np.linalg.inv(Sigma_shared)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Definimos la función discriminante\\n\",\n",
    "    \"def g(x, mu, Sigma_inv, prior):\\n\",\n",
    "    \"    diff = x - mu\\n\",\n",
    "    \"    return -0.5 * np.dot(np.dot(diff, Sigma_inv), diff.T) + np.log(prior)\\n\",\n",
    "    \"\\n\",\n",
    "    \"prior1 = prior2 = 0.5\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Creamos una malla para visualizar la frontera de decisión\\n\",\n",
    "    \"x_min, x_max = X_bayes[:, 0].min() - 1, X_bayes[:, 0].max() + 1\\n\",\n",
    "    \"y_min, y_max = X_bayes[:, 1].min() - 1, X_bayes[:, 1].max() + 1\\n\",\n",
    "    \"xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\\n\",\n",
    "    \"grid = np.c_[xx.ravel(), yy.ravel()]\\n\",\n",
    "    \"\\n\",\n",
    "    \"g1 = np.array([g(x, mu1_est, Sigma_inv, prior1) for x in grid])\\n\",\n",
    "    \"g2 = np.array([g(x, mu2_est, Sigma_inv, prior2) for x in grid])\\n\",\n",
    "    \"\\n\",\n",
    "    \"decision = (g1 - g2).reshape(xx.shape)  \\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8,6))\\n\",\n",
    "    \"plt.contourf(xx, yy, decision, levels=[-np.inf, 0, np.inf], alpha=0.3, cmap=ListedColormap(['blue', 'red']))\\n\",\n",
    "    \"plt.scatter(X1[:,0], X1[:,1], c='blue', label='Clase 1')\\n\",\n",
    "    \"plt.scatter(X2[:,0], X2[:,1], c='red', label='Clase 2')\\n\",\n",
    "    \"plt.title('Frontera de decisión Bayesiana (covarianza compartida)')\\n\",\n",
    "    \"plt.xlabel('x1')\\n\",\n",
    "    \"plt.ylabel('x2')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comentario: Este ejemplo ilustra cómo la regla de Bayes se traduce en una frontera de decisión óptima en el contexto de pdfs gaussianas.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e2ef6f26-64d7-46a1-9170-1a0860a9fa1b\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Conclusiones\\n\",\n",
    "    \"\\n\",\n",
    "    \"En este cuaderno se han abordado temas esenciales en IA sub-simbólica:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Distribuciones Gaussianas:** Fundamentos matemáticos y visualización en 1D y 2D, lo cual es base para muchos modelos probabilísticos.\\n\",\n",
    "    \"- **Reconocimiento de Formas y Extracción de Características:** Procesos clave para transformar datos reales en vectores útiles para clasificación y otras tareas.\\n\",\n",
    "    \"- **Estimación No Paramétrica de Densidades:** Métodos como Ventana de Parzen y k-NN que permiten obtener una estimación directa de \\\\(p(x)\\\\) sin suponer una forma predeterminada.\\n\",\n",
    "    \"- **Estimación Paramétrica de Densidades:** Uso de la máxima verosimilitud para obtener parámetros de distribuciones, importante cuando se conoce la forma de la pdf.\\n\",\n",
    "    \"- **Redes Neuronales para Estimación de Densidades (PNN):** Combina lo mejor de los métodos no paramétricos con la capacidad de generalización de las ANN.\\n\",\n",
    "    \"- **Fundamentos de las ANN:** Arquitectura, dinámica y aprendizaje que permiten aproximar funciones complejas y resolver tareas de clasificación y regresión.\\n\",\n",
    "    \"- **Clasificadores Bayesianos:** Aplicación del Teorema de Bayes para la toma de decisiones óptima, que se traduce en fronteras de decisión con mínima probabilidad de error.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Cada ejemplo ha sido acompañado de código comentado que ilustra la teoría y demuestra cómo llevarla a la práctica. La comprensión de estos fundamentos es esencial para desarrollar sistemas de IA robustos y eficientes en el mundo real.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.x\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "c553a3922285c81a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'id': '78e9a445-6e97-4a8a-8f95-03c430c9882b',\n",
       "   'metadata': {},\n",
       "   'source': ['# Cuaderno Extendido: Teoría y Ejemplos en IA Sub-Simbólica\\n',\n",
       "    '\\n',\n",
       "    'Este cuaderno reúne de manera detallada los fundamentos teóricos y prácticos de varios temas clave en IA sub-simbólica. Se abordan:\\n',\n",
       "    '\\n',\n",
       "    '- Distribuciones Gaussianas (unidimensional y multivariante)\\n',\n",
       "    '- Reconocimiento de formas y extracción de características\\n',\n",
       "    '- Estimación no paramétrica de densidades: método de Ventana de Parzen y k-Nearest Neighbors (k-NN)\\n',\n",
       "    '- Estimación paramétrica de densidades (máxima verosimilitud)\\n',\n",
       "    '- Uso de redes neuronales para estimar densidades (Parzen Neural Networks – PNN)\\n',\n",
       "    '- Fundamentos de las redes neuronales artificiales (ANN)\\n',\n",
       "    '- Clasificadores bayesianos y toma de decisiones\\n',\n",
       "    '\\n',\n",
       "    'Cada sección contiene una explicación teórica completa y ejemplos de código con su respectiva justificación, para que puedas comprender en profundidad cada tema.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'cf6d113c-5b32-4e0f-b0fe-0b6d142b25b5',\n",
       "   'metadata': {},\n",
       "   'source': ['## 1. Distribuciones Gaussianas y sus Propiedades\\n',\n",
       "    '\\n',\n",
       "    'Las distribuciones gaussianas (o normales) son esenciales en estadística y en el procesamiento de señales. \\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    '#### Distribución Normal Unidimensional:\\n',\n",
       "    '\\n',\n",
       "    'La función de densidad de probabilidad (pdf) de una variable aleatoria \\\\(X\\\\) con distribución normal es:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'p(x) = \\\\frac{1}{\\\\sigma \\\\sqrt{2\\\\pi}} \\\\exp\\\\left(-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'donde \\\\(\\\\mu\\\\) es la media y \\\\(\\\\sigma^2\\\\) es la varianza.\\n',\n",
       "    '\\n',\n",
       "    '#### Distribución Normal Multivariante:\\n',\n",
       "    '\\n',\n",
       "    'Para un vector aleatorio \\\\(x \\\\in \\\\mathbb{R}^d\\\\), la pdf es:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'p(x) = \\\\frac{1}{\\\\sqrt{(2\\\\pi)^d |\\\\Sigma|}} \\\\exp\\\\left(-\\\\frac{1}{2}(x-\\\\mu)^T\\\\Sigma^{-1}(x-\\\\mu)\\\\right)\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'donde \\\\(\\\\mu\\\\) es el vector de medias y \\\\(\\\\Sigma\\\\) la matriz de covarianza. Los contornos de nivel (donde \\\\(p(x)\\\\) es constante) son elipses (o hiperelipsoides) que se alinean con los autovectores de \\\\(\\\\Sigma\\\\).\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'Conocer las propiedades de la distribución normal es fundamental para modelar la incertidumbre en los datos, estimar parámetros en modelos paramétricos y construir clasificadores basados en la distancia (como los basados en la distancia de Mahalanobis).']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '04c95d2b-1e32-42e2-93a0-0f64b6d846aa',\n",
       "   'metadata': {},\n",
       "   'source': ['# Ejemplo 1: Visualización de una distribución normal unidimensional\\n',\n",
       "    'import numpy as np\\n',\n",
       "    'import matplotlib.pyplot as plt\\n',\n",
       "    '\\n',\n",
       "    '# Parámetros de la distribución normal\\n',\n",
       "    'mu = 0\\n',\n",
       "    'sigma = 1\\n',\n",
       "    '\\n',\n",
       "    '# Generamos un rango de valores para x\\n',\n",
       "    'x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\\n',\n",
       "    'pdf = (1/(sigma * np.sqrt(2*np.pi))) * np.exp(-((x-mu)**2)/(2*sigma**2))\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8, 4))\\n',\n",
       "    \"plt.plot(x, pdf, label='N(0,1)')\\n\",\n",
       "    \"plt.title('Distribución Normal Unidimensional')\\n\",\n",
       "    \"plt.xlabel('x')\\n\",\n",
       "    \"plt.ylabel('Densidad')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Este ejemplo muestra cómo se comporta una normal estándar, lo cual es la base para muchos modelos.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'e2d3c453-f640-4f27-85ef-35c1deceda45',\n",
       "   'metadata': {},\n",
       "   'source': ['### Ejemplo 2: Contornos de una Distribución Gaussiana Multivariante (2D)\\n',\n",
       "    '\\n',\n",
       "    'En 2D, los contornos de la densidad son elipses. Estos contornos están determinados por la matriz de covarianza y reflejan la correlación entre las variables. En el siguiente ejemplo se plotean los contornos de una distribución 2D usando una elipse que se calcula a partir de los autovalores y autovectores de \\\\(\\\\Sigma\\\\).']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '8f742733-1d38-4bd0-a5d6-90fa43e78033',\n",
       "   'metadata': {},\n",
       "   'source': ['from matplotlib.patches import Ellipse\\n',\n",
       "    '\\n',\n",
       "    \"def plot_gaussian_contours(mu, Sigma, ax, n_std=2.0, facecolor='none', **kwargs):\\n\",\n",
       "    '    # Calcula autovalores y autovectores\\n',\n",
       "    '    vals, vecs = np.linalg.eigh(Sigma)\\n',\n",
       "    '    order = vals.argsort()[::-1]\\n',\n",
       "    '    vals, vecs = vals[order], vecs[:, order]\\n',\n",
       "    '    \\n',\n",
       "    '    # Ángulo de rotación\\n',\n",
       "    '    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\\n',\n",
       "    '    \\n',\n",
       "    '    # Ancho y alto de la elipse\\n',\n",
       "    '    width, height = 2 * n_std * np.sqrt(vals)\\n',\n",
       "    '    ellip = Ellipse(xy=mu, width=width, height=height, angle=theta, facecolor=facecolor, **kwargs)\\n',\n",
       "    '    ax.add_patch(ellip)\\n',\n",
       "    '    return ellip\\n',\n",
       "    '\\n',\n",
       "    '# Parámetros para la gaussiana 2D\\n',\n",
       "    'mu2d = np.array([1, 2])\\n',\n",
       "    'Sigma2d = np.array([[3, 1], [1, 2]])\\n',\n",
       "    '\\n',\n",
       "    'x, y = np.mgrid[-2:6:.01, -1:7:.01]\\n',\n",
       "    'pos = np.dstack((x, y))\\n',\n",
       "    '\\n',\n",
       "    'def multivariate_gaussian(pos, mu, Sigma):\\n',\n",
       "    '    n = mu.shape[0]\\n',\n",
       "    '    Sigma_det = np.linalg.det(Sigma)\\n',\n",
       "    '    Sigma_inv = np.linalg.inv(Sigma)\\n',\n",
       "    '    N = np.sqrt((2*np.pi)**n * Sigma_det)\\n',\n",
       "    '    \\n',\n",
       "    \"    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\\n\",\n",
       "    '    return np.exp(-fac/2) / N\\n',\n",
       "    '\\n',\n",
       "    'Z = multivariate_gaussian(pos, mu2d, Sigma2d)\\n',\n",
       "    '\\n',\n",
       "    'fig, ax = plt.subplots(figsize=(8, 6))\\n',\n",
       "    \"cf = ax.contourf(x, y, Z, cmap='viridis')\\n\",\n",
       "    'plt.colorbar(cf)\\n',\n",
       "    \"plot_gaussian_contours(mu2d, Sigma2d, ax, n_std=2, edgecolor='red')\\n\",\n",
       "    \"ax.set_title('Contornos de una Distribución Gaussiana 2D')\\n\",\n",
       "    \"ax.set_xlabel('x')\\n\",\n",
       "    \"ax.set_ylabel('y')\\n\",\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Este código muestra visualmente cómo la matriz de covarianza afecta la forma de la distribución.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': '3164cfed-ff85-42dc-9691-2f485d46f341',\n",
       "   'metadata': {},\n",
       "   'source': ['## 2. Reconocimiento de Formas y Extracción de Características\\n',\n",
       "    '\\n',\n",
       "    'En sistemas de IA, el reconocimiento de patrones implica convertir señales del mundo real en vectores de características que luego se usan para clasificar o identificar objetos. \\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    'El proceso general incluye:\\n',\n",
       "    '\\n',\n",
       "    '1. **Percepción:** Se capta el estímulo (por ejemplo, una imagen o un sonido) y se digitaliza.\\n',\n",
       "    '2. **Extracción de Características:** Se procesan los datos para extraer información relevante (por ejemplo, contornos, píxeles negros, medidas geométricas, etc.).\\n',\n",
       "    '3. **Clasificación:** Se utiliza un clasificador (por ejemplo, basado en redes neuronales o métodos estadísticos) para asignar la entrada a una clase.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'La calidad de las características extraídas es crucial; características bien elegidas permiten reducir la dimensionalidad del problema y mejorar la precisión del sistema de reconocimiento.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'A continuación se simula la extracción de una característica sencilla (por ejemplo, la intensidad media) a partir de datos sintéticos para dos clases.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '1f8f33d8-8233-44d9-a4af-0035e8bcae53',\n",
       "   'metadata': {},\n",
       "   'source': ['# Ejemplo: Extracción de una característica y visualización de un histograma\\n',\n",
       "    'np.random.seed(0)\\n',\n",
       "    'n_samples = 100\\n',\n",
       "    '\\n',\n",
       "    '# Simulamos dos clases con diferentes medias\\n',\n",
       "    'features_class1 = np.random.normal(loc=3, scale=0.5, size=(n_samples, 1))\\n',\n",
       "    'features_class2 = np.random.normal(loc=7, scale=0.5, size=(n_samples, 1))\\n',\n",
       "    '\\n',\n",
       "    '# Etiquetas\\n',\n",
       "    'labels_class1 = np.zeros((n_samples,))\\n',\n",
       "    'labels_class2 = np.ones((n_samples,))\\n',\n",
       "    '\\n',\n",
       "    '# Unimos los datos\\n',\n",
       "    'X_features = np.vstack((features_class1, features_class2))\\n',\n",
       "    'y_labels = np.hstack((labels_class1, labels_class2))\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8,4))\\n',\n",
       "    \"plt.hist(features_class1, bins=20, alpha=0.7, label='Clase 1')\\n\",\n",
       "    \"plt.hist(features_class2, bins=20, alpha=0.7, label='Clase 2')\\n\",\n",
       "    \"plt.title('Histograma de la característica extraída')\\n\",\n",
       "    \"plt.xlabel('Valor de la característica')\\n\",\n",
       "    \"plt.ylabel('Frecuencia')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: Este ejemplo ilustra cómo, aun una característica simple, puede separar dos clases.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'e9ab5c3e-26a8-4413-b73d-d7afc4e04a38',\n",
       "   'metadata': {},\n",
       "   'source': ['## 3. Estimación No Paramétrica de Densidades: Ventana de Parzen\\n',\n",
       "    '\\n',\n",
       "    'La estimación de densidad no paramétrica permite calcular \\\\(p(x)\\\\) directamente a partir de los datos sin asumir una forma fija para la distribución. \\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    'El método de la Ventana de Parzen consiste en definir un kernel (por ejemplo, un indicador de un intervalo o un kernel gaussiano) y estimar:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'p(x_0) \\\\approx \\\\frac{k}{n\\\\, V},\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'donde \\\\(k\\\\) es el número de muestras que caen en la ventana centrada en \\\\(x_0\\\\), \\\\(n\\\\) es el número total de muestras, y \\\\(V\\\\) es el volumen (en 1D, el ancho) de la ventana.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'Este método es muy flexible, ya que no impone una forma predefinida a la pdf, pero su desempeño depende fuertemente de la elección del ancho \\\\(h\\\\) del kernel.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'Se implementa una función en 1D que calcula la densidad usando un kernel de ventana rectangular.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '3cbdebf5-3235-45ff-a78a-7e18b9fbb1ac',\n",
       "   'metadata': {},\n",
       "   'source': ['def parzen_window_estimator(x0, data, h):\\n',\n",
       "    '    # Ventana rectangular de ancho h (volumen en 1D es h)\\n',\n",
       "    '    n = len(data)\\n',\n",
       "    '    kernel_values = np.where(np.abs(data - x0) <= h/2, 1, 0)\\n',\n",
       "    '    return np.sum(kernel_values) / (n * h)\\n',\n",
       "    '\\n',\n",
       "    '# Generamos datos sintéticos: mezcla de dos distribuciones\\n',\n",
       "    'data_parzen = np.concatenate((np.random.normal(2, 0.3, 500), np.random.normal(5, 0.5, 500)))\\n',\n",
       "    'x_vals = np.linspace(0, 7, 200)\\n',\n",
       "    'density_est_parzen = [parzen_window_estimator(x, data_parzen, h=0.5) for x in x_vals]\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8,4))\\n',\n",
       "    \"plt.plot(x_vals, density_est_parzen, label='Estimación Parzen')\\n\",\n",
       "    \"plt.title('Estimación de densidad usando Ventana de Parzen')\\n\",\n",
       "    \"plt.xlabel('x')\\n\",\n",
       "    \"plt.ylabel('Densidad estimada')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: La elección de h afecta la suavidad de la estimación. Si h es muy grande se promedia en exceso, y si es muy pequeño la estimación es ruidosa.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'f88bb3e8-45d9-4d66-bb8d-d3f93bb2c0a1',\n",
       "   'metadata': {},\n",
       "   'source': ['## 4. Estimación No Paramétrica: k-Nearest Neighbors (k-NN)\\n',\n",
       "    '\\n',\n",
       "    'El método k-NN para estimación de densidad fija un número de vecinos \\\\(k\\\\) y determina el volumen \\\\(V\\\\) mínimo que contiene esos vecinos. La densidad se estima como:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'p(x_0) \\\\approx \\\\frac{k}{n\\\\, V},\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    'Se debe elegir \\\\(k\\\\) de modo que \\\\(k \\\\to \\\\infty\\\\) y \\\\(k/n \\\\to 0\\\\) cuando \\\\(n \\\\to \\\\infty\\\\), lo que garantiza la consistencia del estimador.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'La ventaja del k-NN es que adapta el volumen de estimación a la densidad local de los datos. En regiones de alta densidad, el volumen será pequeño, lo que mejora la precisión de la estimación.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'Se implementa una función para estimar la densidad en 1D usando el método k-NN.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '7a82e3a9-2a55-4c9a-9a0a-4b63bdc5b67d',\n",
       "   'metadata': {},\n",
       "   'source': ['def knn_density_estimator(x0, data, k):\\n',\n",
       "    '    # Calcula la distancia absoluta a todos los puntos\\n',\n",
       "    '    distances = np.abs(data - x0)\\n',\n",
       "    '    sorted_distances = np.sort(distances)\\n',\n",
       "    \"    # En 1D, el 'volumen' es 2 * la k-ésima distancia\\n\",\n",
       "    '    radius = sorted_distances[k-1]\\n',\n",
       "    '    V = 2 * radius\\n',\n",
       "    '    return k / (len(data) * V)\\n',\n",
       "    '\\n',\n",
       "    'k_val = int(np.sqrt(len(data_parzen)))\\n',\n",
       "    'density_est_knn = [knn_density_estimator(x, data_parzen, k_val) for x in x_vals]\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8,4))\\n',\n",
       "    \"plt.plot(x_vals, density_est_knn, label=f'k-NN (k={k_val})', color='orange')\\n\",\n",
       "    \"plt.title('Estimación de densidad usando k-Nearest Neighbors')\\n\",\n",
       "    \"plt.xlabel('x')\\n\",\n",
       "    \"plt.ylabel('Densidad estimada')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: k-NN adapta la ventana al nivel local de densidad, mejorando la estimación en regiones de alta variabilidad.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'd329f3b2-75a7-4a72-8884-fb8b27597270',\n",
       "   'metadata': {},\n",
       "   'source': ['## 5. Estimación Paramétrica de Densidades\\n',\n",
       "    '\\n',\n",
       "    'En el enfoque paramétrico se asume que la forma de la pdf es conocida (por ejemplo, gaussiana) y se estima sus parámetros (media, varianza, etc.) mediante el método de máxima verosimilitud (ML).\\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    'Para una distribución normal unidimensional, la función de verosimilitud dada \\\\(n\\\\) muestras \\\\(y_1,\\\\ldots,y_n\\\\) es:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'L(\\\\mu,\\\\sigma^2) = \\\\prod_{k=1}^{n} \\\\frac{1}{\\\\sqrt{2\\\\pi \\\\sigma^2}} \\\\exp\\\\left(-\\\\frac{(y_k-\\\\mu)^2}{2\\\\sigma^2}\\\\right)\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'Tomando el logaritmo y derivando se obtiene que el estimador de ML para la media es la media muestral y para la varianza es la varianza muestral.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'Estos estimadores son fundamentales en modelos paramétricos, ya que permiten modelar la pdf con una forma cerrada y obtener soluciones óptimas cuando la suposición es correcta.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'Se generan datos sintéticos de una normal y se calculan la media y varianza muestrales.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '4dce4d16-f858-431a-8a9f-76329f2d49e0',\n",
       "   'metadata': {},\n",
       "   'source': ['# Generamos datos sintéticos de una normal\\n',\n",
       "    'data_gauss = np.random.normal(loc=5, scale=2, size=1000)\\n',\n",
       "    '\\n',\n",
       "    '# Estimación de parámetros (MLE)\\n',\n",
       "    'mu_mle = np.mean(data_gauss)\\n',\n",
       "    'sigma2_mle = np.mean((data_gauss - mu_mle)**2)\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Media estimada (MLE): {mu_mle:.3f}\")\\n',\n",
       "    'print(f\"Varianza estimada (MLE): {sigma2_mle:.3f}\")\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: La media y la varianza muestral son los estimadores de ML para una distribución normal.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'c75d31d0-7d36-4b07-a646-9667b9c20cb1',\n",
       "   'metadata': {},\n",
       "   'source': ['## 6. Redes Neuronales para Estimación de Densidades (Parzen Neural Networks – PNN)\\n',\n",
       "    '\\n',\n",
       "    'La idea es entrenar un perceptrón multicapa (MLP) para aproximar la función de densidad \\\\(p(x)\\\\). Para ello, se generan objetivos (targets) mediante el método de Parzen, y luego se entrena el MLP para que aprenda la relación \\\\(x \\\\to p(x)\\\\).\\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    '1. Se generan datos \\\\(x_i\\\\) de la distribución desconocida.\\n',\n",
       "    '2. Para cada \\\\(x_i\\\\), se estima un valor objetivo \\\\(y_i\\\\) usando la Ventana de Parzen.\\n',\n",
       "    '3. Se entrena un MLP con pares \\\\((x_i, y_i)\\\\). La salida del MLP será la densidad estimada \\\\(\\\\hat{p}(x)\\\\).\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'Este enfoque permite obtener un modelo compacto de la densidad que generaliza a nuevos datos y reduce la necesidad de almacenar toda la muestra, a diferencia de los métodos puramente basados en memoria (Parzen o k-NN). \\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'Se genera una muestra a partir de una distribución exponencial y se entrena un MLP para aproximar la densidad.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': 'eb1ac80b-1b46-4a2f-99a8-48d53ab359d9',\n",
       "   'metadata': {},\n",
       "   'source': ['import tensorflow as tf\\n',\n",
       "    'from tensorflow.keras.models import Sequential\\n',\n",
       "    'from tensorflow.keras.layers import Dense\\n',\n",
       "    '\\n',\n",
       "    '# Generamos datos de una distribución exponencial\\n',\n",
       "    'n_samples = 1000\\n',\n",
       "    'data_exp = np.random.exponential(scale=1.0, size=n_samples)\\n',\n",
       "    '\\n',\n",
       "    '# Usamos la función parzen_window_estimator para generar targets\\n',\n",
       "    'h_value = 0.2\\n',\n",
       "    'X_train = data_exp.reshape(-1, 1)\\n',\n",
       "    'y_train = np.array([parzen_window_estimator(x, data_exp, h_value) for x in data_exp])\\n',\n",
       "    '\\n',\n",
       "    '# Definimos un MLP sencillo\\n',\n",
       "    'model_pnn = Sequential([\\n',\n",
       "    \"    Dense(16, activation='relu', input_shape=(1,)),\\n\",\n",
       "    \"    Dense(16, activation='relu'),\\n\",\n",
       "    \"    Dense(1, activation='relu')  # Forzamos salida no negativa\\n\",\n",
       "    '])\\n',\n",
       "    '\\n',\n",
       "    \"model_pnn.compile(optimizer='adam', loss='mse')\\n\",\n",
       "    '\\n',\n",
       "    '# Entrenamos el MLP para aproximar la densidad\\n',\n",
       "    'history_pnn = model_pnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\\n',\n",
       "    '\\n',\n",
       "    '# Evaluamos la densidad estimada en un rango\\n',\n",
       "    'x_test = np.linspace(0, np.max(data_exp)*1.1, 200).reshape(-1, 1)\\n',\n",
       "    'y_pred = model_pnn.predict(x_test)\\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8,4))\\n',\n",
       "    \"plt.plot(x_test, y_pred, label='Estimación MLP (PNN)')\\n\",\n",
       "    'parzen_vals = [parzen_window_estimator(x, data_exp, h_value) for x in x_test.flatten()]\\n',\n",
       "    \"plt.plot(x_test, parzen_vals, '--', label='Parzen Window')\\n\",\n",
       "    \"plt.title('Comparación: PNN vs Ventana de Parzen')\\n\",\n",
       "    \"plt.xlabel('x')\\n\",\n",
       "    \"plt.ylabel('Densidad estimada')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: El MLP aprende una función continua que aproxima la pdf, lo que es ventajoso en cuanto a generalización.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': '6a57c80a-ec5f-45f6-a817-fdfd680b7a07',\n",
       "   'metadata': {},\n",
       "   'source': ['## 7. Fundamentos de las Redes Neuronales Artificiales (ANN)\\n',\n",
       "    '\\n',\n",
       "    'Las redes neuronales son máquinas de procesamiento inspiradas en el cerebro. Sus elementos fundamentales son:\\n',\n",
       "    '\\n',\n",
       "    '- **Arquitectura:** Organizada en capas (entrada, ocultas, salida). Cada conexión tiene un peso.\\n',\n",
       "    '- **Dinámica:** La señal se propaga a través de la red; cada neurona aplica una función de activación (por ejemplo, sigmoide, ReLU).\\n',\n",
       "    '- **Aprendizaje:** Ajuste de los pesos mediante algoritmos como la retropropagación (backpropagation), minimizando una función de error.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'Comprender la teoría detrás de las ANN es crucial para diseñar y entrenar modelos que puedan aproximar funciones complejas, realizar clasificación o regresión, y extraer características de los datos.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'A continuación se entrena un MLP sencillo en el conjunto de datos Iris para clasificar flores.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': '62ed5d68-9d20-40a9-8a75-2590f50d4764',\n",
       "   'metadata': {},\n",
       "   'source': ['from sklearn.datasets import load_iris\\n',\n",
       "    'from sklearn.model_selection import train_test_split\\n',\n",
       "    'from sklearn.preprocessing import StandardScaler\\n',\n",
       "    'from tensorflow.keras.utils import to_categorical\\n',\n",
       "    '\\n',\n",
       "    '# Cargamos el conjunto de datos Iris\\n',\n",
       "    'iris = load_iris()\\n',\n",
       "    'X_iris = iris.data\\n',\n",
       "    'y_iris = iris.target\\n',\n",
       "    '\\n',\n",
       "    '# Escalamos las características\\n',\n",
       "    'scaler = StandardScaler()\\n',\n",
       "    'X_scaled = scaler.fit_transform(X_iris)\\n',\n",
       "    '\\n',\n",
       "    '# Convertir etiquetas a one-hot\\n',\n",
       "    'y_onehot = to_categorical(y_iris)\\n',\n",
       "    '\\n',\n",
       "    '# Dividimos en entrenamiento y prueba\\n',\n",
       "    'X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.2, random_state=42)\\n',\n",
       "    '\\n',\n",
       "    '# Definimos un MLP sencillo\\n',\n",
       "    'model_iris = Sequential([\\n',\n",
       "    \"    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\\n\",\n",
       "    \"    Dense(16, activation='relu'),\\n\",\n",
       "    \"    Dense(3, activation='softmax')\\n\",\n",
       "    '])\\n',\n",
       "    '\\n',\n",
       "    \"model_iris.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\\n\",\n",
       "    '\\n',\n",
       "    'history_iris = model_iris.fit(X_train, y_train, epochs=100, batch_size=16, verbose=0)\\n',\n",
       "    '\\n',\n",
       "    'loss, acc = model_iris.evaluate(X_test, y_test, verbose=0)\\n',\n",
       "    'print(f\"Precisión en Iris: {acc*100:.2f}%\")\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: Este ejemplo muestra cómo aplicar el algoritmo de retropropagación para entrenar un MLP en un problema de clasificación real.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'eafcb8ed-6679-430f-bd55-4a18a07a60b8',\n",
       "   'metadata': {},\n",
       "   'source': ['## 8. Clasificadores Bayesianos y Toma de Decisiones\\n',\n",
       "    '\\n',\n",
       "    'La teoría bayesiana para la clasificación se basa en el Teorema de Bayes:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'P(\\\\omega_i|x) = \\\\frac{p(x|\\\\omega_i) P(\\\\omega_i)}{p(x)}\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'La regla de decisión de Bayes asigna un patrón \\\\(x\\\\) a la clase \\\\(\\\\omega_i\\\\) que maximice \\\\(P(\\\\omega_i|x)\\\\).\\n',\n",
       "    '\\n',\n",
       "    '### Teoría\\n',\n",
       "    '\\n',\n",
       "    'Si se asume que \\\\(p(x|\\\\omega_i)\\\\) es una pdf gaussiana, se pueden derivar funciones discriminantes. Por ejemplo, tomando el logaritmo:\\n',\n",
       "    '\\n',\n",
       "    '\\\\[\\n',\n",
       "    'g_i(x) = \\\\log p(x|\\\\omega_i) + \\\\log P(\\\\omega_i)\\\\n',\n",
       "    '\\\\]\\n',\n",
       "    '\\n',\n",
       "    'Con pdfs gaussianas, se puede llegar a discriminantes lineales (si se asume covarianza compartida) o cuadráticos.\\n',\n",
       "    '\\n',\n",
       "    '### Importancia\\n',\n",
       "    '\\n',\n",
       "    'La clasificación bayesiana es óptima en el sentido de minimizar la probabilidad de error. Conocer esta teoría permite diseñar sistemas de clasificación que aprovechan la información probabilística y las estimaciones de densidad.\\n',\n",
       "    '\\n',\n",
       "    '### Ejemplo\\n',\n",
       "    '\\n',\n",
       "    'Se genera un conjunto de datos 2D para dos clases y se calcula la frontera de decisión usando discriminantes basados en distribuciones gaussianas con covarianza compartida.']},\n",
       "  {'cell_type': 'code',\n",
       "   'id': 'a2949f26-c206-4a53-9d1a-7fdc78b9ea72',\n",
       "   'metadata': {},\n",
       "   'source': ['from matplotlib.colors import ListedColormap\\n',\n",
       "    '\\n',\n",
       "    '# Generamos datos sintéticos para dos clases (2D)\\n',\n",
       "    'np.random.seed(1)\\n',\n",
       "    'n_points = 200\\n',\n",
       "    'mean1 = [2, 2]\\n',\n",
       "    'cov1 = [[1, 0.5], [0.5, 1]]\\n',\n",
       "    'mean2 = [5, 5]\\n',\n",
       "    'cov2 = [[1, -0.3], [-0.3, 1]]\\n',\n",
       "    '\\n',\n",
       "    'X1 = np.random.multivariate_normal(mean1, cov1, n_points)\\n',\n",
       "    'X2 = np.random.multivariate_normal(mean2, cov2, n_points)\\n',\n",
       "    'X_bayes = np.vstack((X1, X2))\\n',\n",
       "    'y_bayes = np.hstack((np.zeros(n_points), np.ones(n_points)))\\n',\n",
       "    '\\n',\n",
       "    '# Estimación de parámetros para cada clase\\n',\n",
       "    'mu1_est = np.mean(X1, axis=0)\\n',\n",
       "    'mu2_est = np.mean(X2, axis=0)\\n',\n",
       "    'Sigma1_est = np.cov(X1, rowvar=False)\\n',\n",
       "    'Sigma2_est = np.cov(X2, rowvar=False)\\n',\n",
       "    '\\n',\n",
       "    '# Asumimos que ambas clases tienen la misma covarianza (promedio)\\n',\n",
       "    'Sigma_shared = (Sigma1_est + Sigma2_est) / 2\\n',\n",
       "    'Sigma_inv = np.linalg.inv(Sigma_shared)\\n',\n",
       "    '\\n',\n",
       "    '# Definimos la función discriminante\\n',\n",
       "    'def g(x, mu, Sigma_inv, prior):\\n',\n",
       "    '    diff = x - mu\\n',\n",
       "    '    return -0.5 * np.dot(np.dot(diff, Sigma_inv), diff.T) + np.log(prior)\\n',\n",
       "    '\\n',\n",
       "    'prior1 = prior2 = 0.5\\n',\n",
       "    '\\n',\n",
       "    '# Creamos una malla para visualizar la frontera de decisión\\n',\n",
       "    'x_min, x_max = X_bayes[:, 0].min() - 1, X_bayes[:, 0].max() + 1\\n',\n",
       "    'y_min, y_max = X_bayes[:, 1].min() - 1, X_bayes[:, 1].max() + 1\\n',\n",
       "    'xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\\n',\n",
       "    'grid = np.c_[xx.ravel(), yy.ravel()]\\n',\n",
       "    '\\n',\n",
       "    'g1 = np.array([g(x, mu1_est, Sigma_inv, prior1) for x in grid])\\n',\n",
       "    'g2 = np.array([g(x, mu2_est, Sigma_inv, prior2) for x in grid])\\n',\n",
       "    '\\n',\n",
       "    'decision = (g1 - g2).reshape(xx.shape)  \\n',\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(8,6))\\n',\n",
       "    \"plt.contourf(xx, yy, decision, levels=[-np.inf, 0, np.inf], alpha=0.3, cmap=ListedColormap(['blue', 'red']))\\n\",\n",
       "    \"plt.scatter(X1[:,0], X1[:,1], c='blue', label='Clase 1')\\n\",\n",
       "    \"plt.scatter(X2[:,0], X2[:,1], c='red', label='Clase 2')\\n\",\n",
       "    \"plt.title('Frontera de decisión Bayesiana (covarianza compartida)')\\n\",\n",
       "    \"plt.xlabel('x1')\\n\",\n",
       "    \"plt.ylabel('x2')\\n\",\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Comentario: Este ejemplo ilustra cómo la regla de Bayes se traduce en una frontera de decisión óptima en el contexto de pdfs gaussianas.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'id': 'e2ef6f26-64d7-46a1-9170-1a0860a9fa1b',\n",
       "   'metadata': {},\n",
       "   'source': ['## Conclusiones\\n',\n",
       "    '\\n',\n",
       "    'En este cuaderno se han abordado temas esenciales en IA sub-simbólica:\\n',\n",
       "    '\\n',\n",
       "    '- **Distribuciones Gaussianas:** Fundamentos matemáticos y visualización en 1D y 2D, lo cual es base para muchos modelos probabilísticos.\\n',\n",
       "    '- **Reconocimiento de Formas y Extracción de Características:** Procesos clave para transformar datos reales en vectores útiles para clasificación y otras tareas.\\n',\n",
       "    '- **Estimación No Paramétrica de Densidades:** Métodos como Ventana de Parzen y k-NN que permiten obtener una estimación directa de \\\\(p(x)\\\\) sin suponer una forma predeterminada.\\n',\n",
       "    '- **Estimación Paramétrica de Densidades:** Uso de la máxima verosimilitud para obtener parámetros de distribuciones, importante cuando se conoce la forma de la pdf.\\n',\n",
       "    '- **Redes Neuronales para Estimación de Densidades (PNN):** Combina lo mejor de los métodos no paramétricos con la capacidad de generalización de las ANN.\\n',\n",
       "    '- **Fundamentos de las ANN:** Arquitectura, dinámica y aprendizaje que permiten aproximar funciones complejas y resolver tareas de clasificación y regresión.\\n',\n",
       "    '- **Clasificadores Bayesianos:** Aplicación del Teorema de Bayes para la toma de decisiones óptima, que se traduce en fronteras de decisión con mínima probabilidad de error.\\n',\n",
       "    '\\n',\n",
       "    'Cada ejemplo ha sido acompañado de código comentado que ilustra la teoría y demuestra cómo llevarla a la práctica. La comprensión de estos fundamentos es esencial para desarrollar sistemas de IA robustos y eficientes en el mundo real.']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'name': 'python', 'version': '3.x'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2573d4140de08c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
